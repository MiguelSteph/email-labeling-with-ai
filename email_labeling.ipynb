{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "11W64ngbVmKQwYAkwlAVZAQ1enaEl3U3o",
      "authorship_tag": "ABX9TyNqe3z0HfhMGUdZG1pDXvs1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MiguelSteph/email-labeling-with-ai/blob/main/email_labeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install missing packages"
      ],
      "metadata": {
        "id": "1GznNj3Hktrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q 'google-api-python-client==1.7.2' 'google-auth-httplib2==0.0.3' 'google-auth-oauthlib==0.4.1' 'google-genai>=1.51.0'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhFTh4A7kxJ5",
        "outputId": "ddff6e1d-6db3-45d1-c42f-f1a7e7a2e900"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.3/53.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.6/426.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.3/233.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires google-auth==2.43.0, but you have google-auth 2.45.0 which is incompatible.\n",
            "pydrive2 1.21.3 requires google-api-python-client>=1.12.5, but you have google-api-python-client 1.7.2 which is incompatible.\n",
            "earthengine-api 1.5.24 requires google-api-python-client>=1.12.1, but you have google-api-python-client 1.7.2 which is incompatible.\n",
            "firebase-admin 6.9.0 requires google-api-python-client>=1.7.8, but you have google-api-python-client 1.7.2 which is incompatible.\n",
            "google-adk 1.21.0 requires google-api-python-client<3.0.0,>=2.157.0, but you have google-api-python-client 1.7.2 which is incompatible.\n",
            "pandas-gbq 0.30.0 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.auth.transport.requests import Request\n",
        "from google.oauth2.credentials import Credentials\n",
        "from google.oauth2 import service_account\n",
        "from google_auth_oauthlib.flow import InstalledAppFlow\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.errors import HttpError\n",
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "from urllib.parse import unquote\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from umap import UMAP\n",
        "from hdbscan import HDBSCAN\n",
        "\n",
        "import base64\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from google.colab import userdata\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Change the credential path to point to your credentials file.\n",
        "CREDENTIALS_FILE_PATH = \"/content/drive/MyDrive/Projects/email_labeling/credentials.json\"\n",
        "RAW_DATASET_PATH = \"/content/drive/MyDrive/Projects/email_labeling/raw_dataset.pkl\"\n",
        "DATASET_WITH_EMBEDDINGS_PATH = \"/content/drive/MyDrive/Projects/email_labeling/dataset_with_embeddings.pkl\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yYiccLDl3nB",
        "outputId": "bcfb11c2-e16a-4b6d-f23a-f28b25a7dd8f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/hdbscan/robust_single_linkage_.py:175: SyntaxWarning: invalid escape sequence '\\{'\n",
            "  $max \\{ core_k(a), core_k(b), 1/\\alpha d(a,b) \\}$.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get user access"
      ],
      "metadata": {
        "id": "enQbbKnrPkeZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SCOPES = [\"https://www.googleapis.com/auth/gmail.readonly\"]\n",
        "\n",
        "def get_user_access():\n",
        "  \"\"\"Get the user's access and refresh tokens\n",
        "  and save them in a token.json file.\n",
        "  \"\"\"\n",
        "  creds = None\n",
        "  if os.path.exists(\"token.json\"):\n",
        "    creds = Credentials.from_authorized_user_file(\"token.json\", SCOPES)\n",
        "  if not creds or not creds.valid:\n",
        "    if creds and creds.expired and creds.refresh_token:\n",
        "      creds.refresh(Request())\n",
        "    else:\n",
        "      flow = InstalledAppFlow.from_client_secrets_file(CREDENTIALS_FILE_PATH,\n",
        "                                                       SCOPES)\n",
        "      # creds = flow.run_local_server(port=0)\n",
        "      creds = flow.run_console()\n",
        "    with open(\"token.json\", \"w\") as token:\n",
        "      token.write(creds.to_json())\n",
        "    return creds\n",
        "\n",
        "\n",
        "# Get credentials\n",
        "creds = get_user_access()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0csq_j6GPnw3",
        "outputId": "08057fc2-b7cb-4878-a4dc-82badae6851c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=201451649192-58mbdfjc2kdgj591d5qj090jditt0rbg.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fgmail.readonly&state=KnEQTzENl9dgGODq453DiX0FDpKzPR&prompt=consent&access_type=offline\n",
            "Enter the authorization code: 4/1ATX87lPo-fj9h9gQvy0aDJNqVAAR6099NAMZguOu13kUv3mizhzy1IuOKJY\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare raw dataset\n",
        "\n",
        "Follow the steps here https://developers.google.com/workspace/gmail/api/quickstart/python to configure and download the credentials."
      ],
      "metadata": {
        "id": "1pSxI20rQiss"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fetch and save emails from Gmails"
      ],
      "metadata": {
        "id": "ANvGFLwMU8d8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_base64_str(encoded_str):\n",
        "  return base64.urlsafe_b64decode(encoded_str + '=' * (4 - len(encoded_str) % 4)).decode('utf-8')\n",
        "\n",
        "\n",
        "def get_email_content(msgs):\n",
        "  for m in msgs:\n",
        "    mimeType = m['mimeType']\n",
        "    if (mimeType == 'text/plain'):\n",
        "      return decode_base64_str(m['body']['data'])\n",
        "\n",
        "    if (mimeType == 'text/html'):\n",
        "      html_text = decode_base64_str(m['body']['data'])\n",
        "      return BeautifulSoup(html_text, 'html.parser').get_text()\n",
        "\n",
        "    if 'parts' in m:\n",
        "      return get_email_content(m['parts'])\n",
        "\n",
        "\n",
        "def get_emails(credentials, num_emails):\n",
        "  max_emails_per_fetch = 500\n",
        "  try:\n",
        "    email_fetched = 0\n",
        "    page_token = \"\"\n",
        "    ids = []\n",
        "    thread_ids = []\n",
        "    emails_content = []\n",
        "    while email_fetched < num_emails:\n",
        "      fetch_count = min(num_emails - email_fetched, max_emails_per_fetch)\n",
        "      service = build(\"gmail\", \"v1\", credentials=credentials)\n",
        "      results = service.users().messages().list(userId=\"me\",\n",
        "                                                labelIds=[\"INBOX\"],\n",
        "                                                includeSpamTrash=False,\n",
        "                                                # maxResults=fetch_count,\n",
        "                                                pageToken=page_token).execute()\n",
        "      messages = results.get(\"messages\", [])\n",
        "      page_token = results.get(\"nextPageToken\", \"\")\n",
        "\n",
        "      for message in messages:\n",
        "        msg =  service.users().messages().get(userId=\"me\", id=message[\"id\"], format=\"full\").execute()\n",
        "        content = get_email_content([msg['payload']])\n",
        "        content = content.replace('\\r', '')\n",
        "        content = content.replace('\\n', '')\n",
        "        ids.append(message[\"id\"])\n",
        "        thread_ids.append(message[\"threadId\"])\n",
        "        emails_content.append(content)\n",
        "\n",
        "      email_fetched += len(messages)\n",
        "    return ids, thread_ids, emails_content\n",
        "  except HttpError as error:\n",
        "    print(f\"An error occurred: {error}\")"
      ],
      "metadata": {
        "id": "8pNMP8HulWsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "  - Fetch emails\n",
        "  - Put them in a dataframe\n",
        "  - Save the dataframe as a pickle file\n",
        "\"\"\"\n",
        "ids, thread_ids, emails_content = get_emails(creds, 1000)\n",
        "df = pd.DataFrame({\n",
        "      \"ids\": ids,\n",
        "      \"thread_ids\": thread_ids,\n",
        "      \"body\": emails_content\n",
        "    })\n",
        "\n",
        "# Save raw dataset\n",
        "df.to_pickle(RAW_DATASET_PATH)"
      ],
      "metadata": {
        "id": "kMNuVArVkoNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and clean raw dataset"
      ],
      "metadata": {
        "id": "eYOMxTLnVHQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_CONTENT_CHARACTERS = 10_000\n",
        "\n",
        "# Load raw dataset\n",
        "df = pd.read_pickle(RAW_DATASET_PATH)\n",
        "\n",
        "# Remove all the links from the emails content\n",
        "df['body'] = df['body'].map(lambda email: unquote(email))\n",
        "df['body'] = df['body'].map(lambda email: re.sub(r'http\\S+', ' ', email))\n",
        "# Remove HTML tag\n",
        "df['body'] = df['body'].map(lambda email: BeautifulSoup(email, 'html.parser').get_text())\n",
        "# Remove consecutive punctuations and replace them with only the first one\n",
        "df['body'] = df['body'].map(lambda email: re.sub(r'(\\W)(?=\\1)', ' ', email))\n",
        "\n",
        "# Remove HTML special characters\n",
        "def remove_hmtl_special_characters(email):\n",
        "  email = email.replace('&#8192;', '')\n",
        "  email = email.replace('&#8193;', '')\n",
        "  email = email.replace('&#8194;', '')\n",
        "  email = email.replace('&#8195;', '')\n",
        "  email = email.replace('&#8196;', '')\n",
        "  email = email.replace('&#8197;', '')\n",
        "  email = email.replace('&#8198;', '')\n",
        "  email = email.replace('&#8199;', '')\n",
        "  email = email.replace('&#8200;', '')\n",
        "  email = email.replace('&#8201;', '')\n",
        "  email = email.replace('&#8202;', '')\n",
        "  email = email.replace('&#8203;', '')\n",
        "  email = email.replace('&#8204;', '')\n",
        "  email = email.replace('&#8205;', '')\n",
        "  email = email.replace('&#8206;', '')\n",
        "  email = email.replace('&#8207;', '')\n",
        "  email = email.replace('&#8239;', '')\n",
        "  email = email.replace('&zwnj;', '')\n",
        "  email = email.replace('&#847;', '')\n",
        "  return email\n",
        "\n",
        "df['body'] = df['body'].map(remove_hmtl_special_characters)\n",
        "\n",
        "# Replace multiple spaces with a single one\n",
        "df['body'] = df['body'].map(lambda email: ' '.join(email.split()))\n",
        "\n",
        "# Removes rows with empty or very small email body\n",
        "df['body'] = df['body'].map(lambda email: email if len(email) > 40 else '')\n",
        "empty_content_indexes = df[df['body'] == ''].index.tolist()\n",
        "df = df.drop(labels=empty_content_indexes)\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "# Clip the email body to maximum 10_000 characters\n",
        "df['body'] = df['body'].map(lambda email: email if len(email) < MAX_CONTENT_CHARACTERS else email[:MAX_CONTENT_CHARACTERS])"
      ],
      "metadata": {
        "id": "JM3Dc8-9fD6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Select the first 5 emails for testing purposes\n",
        "# df = df[:10]\n",
        "# print(df)"
      ],
      "metadata": {
        "id": "Ir4BfClyQ-3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embed raw emails"
      ],
      "metadata": {
        "id": "8F9w92YvC5cb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "# GEMINI_API_KEY = userdata.get('GEMINI_PAID_TIER_API_KEY')\n",
        "MODEL_NAME = \"gemini-embedding-001\"\n",
        "client = genai.Client(api_key=GEMINI_API_KEY)\n",
        "\n",
        "def count_tokens(contents):\n",
        "  result = client.models.count_tokens(\n",
        "      model=MODEL_NAME,\n",
        "      contents=contents,\n",
        "  )\n",
        "  return result.total_tokens\n",
        "\n",
        "def get_embedding(contents):\n",
        "  result = client.models.embed_content(\n",
        "      model=MODEL_NAME,\n",
        "      contents=contents,\n",
        "      config=types.EmbedContentConfig(task_type=\"CLUSTERING\")\n",
        "  )\n",
        "  embeddings = [embedding.values for embedding in result.embeddings]\n",
        "  return np.array(embeddings).mean(axis=0)"
      ],
      "metadata": {
        "id": "0z5nrkn9C5J3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Counting tokens"
      ],
      "metadata": {
        "id": "NS-SLCX3xDTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_CHUNK_CHARACTERS = 2048\n",
        "\n",
        "def maybe_split_content(content):\n",
        "  content_len = len(content)\n",
        "  if content_len <= MAX_CHUNK_CHARACTERS:\n",
        "    return [content]\n",
        "  words_chunk = content.split()\n",
        "  chunks = []\n",
        "  current_chunks = []\n",
        "  current_len = 0\n",
        "  for word in words_chunk:\n",
        "    word_len = len(word)\n",
        "    if current_len + word_len <= MAX_CHUNK_CHARACTERS:\n",
        "      current_chunks.append(word)\n",
        "      current_len += word_len\n",
        "    else:\n",
        "      chunk = ' '.join(current_chunks)\n",
        "      chunks.append(chunk)\n",
        "      current_chunks = [word]\n",
        "      current_len = word_len\n",
        "  if len(current_chunks) > 0:\n",
        "    chunk = ' '.join(current_chunks)\n",
        "    chunks.append(chunk)\n",
        "  return chunks\n",
        "\n",
        "\n",
        "\n",
        "tokens_counts = []\n",
        "total_tokens = 0\n",
        "for email in df['body']:\n",
        "  contents = maybe_split_content(email)\n",
        "  request_token_count = count_tokens(contents)\n",
        "  total_tokens += request_token_count\n",
        "  tokens_counts.append(request_token_count)\n",
        "\n",
        "print(f\"Max token per request is {max(tokens_counts)}\")\n",
        "print(f\"Total tokens count is {total_tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0JsZE3vxNkw",
        "outputId": "ff9902d0-46a0-4069-a857-f6d76e152310"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max token per request is 4612\n",
            "Total tokens count is 651929\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get embeddings"
      ],
      "metadata": {
        "id": "ZSZ7v-d6pRtu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = []\n",
        "error_indexes = []\n",
        "\n",
        "for idx, email in enumerate(df['body']):\n",
        "  try:\n",
        "    contents = maybe_split_content(email)\n",
        "    embedding = get_embedding(contents)\n",
        "    embeddings.append(embedding)\n",
        "  except:\n",
        "    error_indexes.append(idx)\n",
        "\n",
        "  # Pause for 1 minute after 120 requests\n",
        "  if idx > 0 and idx%120 == 0:\n",
        "    time.sleep(60)\n",
        "\n",
        "# Drop the emails for which we fail to get the embeddings\n",
        "df = df.drop(labels=error_indexes)\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "# Add the embeddings in the dataframe\n",
        "df['embedding'] = embeddings\n",
        "\n",
        "# Save dataset with embeddings\n",
        "df.to_pickle(DATASET_WITH_EMBEDDINGS_PATH)\n",
        "\n",
        "if len(error_indexes) > 0:\n",
        "  print(f\"The following rows have been removed: {error_indexes}\")\n",
        "else:\n",
        "  print(f\"No row have been removed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNlZRP5w8VpI",
        "outputId": "65e17350-b58a-4b08-e8d7-36617cacf323"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No row have been removed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cluster Emails"
      ],
      "metadata": {
        "id": "USsDeo7B7t8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "REDUCED_DIM = 10\n",
        "SEED = 42\n",
        "\n",
        "# Load emails with embeddings\n",
        "df = pd.read_pickle(DATASET_WITH_EMBEDDINGS_PATH)\n",
        "\n",
        "# Reduce the dimensionality of the emails\n",
        "umap_model = UMAP(n_components=REDUCED_DIM, min_dist=0.0,\n",
        "                  metric='cosine', random_state=SEED)\n",
        "reduced_embeddings = umap_model.fit_transform(df['embedding'].tolist())\n",
        "\n",
        "# Cluster the emails\n",
        "hdbscan_model = HDBSCAN(min_cluster_size=40,\n",
        "                        metric='euclidean',\n",
        "                        cluster_selection_method='eom').fit(reduced_embeddings)\n",
        "clusters = hdbscan_model.labels_\n",
        "clusters = [int(cluster_id) for cluster_id in clusters]\n",
        "\n",
        "df['cluster_id'] = clusters\n",
        "print(f\"There are {len(set(clusters))} clusters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGmcT4B67VIe",
        "outputId": "8bcd6fbe-4991-48f3-8303-e9a271c36700"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 8 clusters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main topics per cluster"
      ],
      "metadata": {
        "id": "oTe4-oR25Jbl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the proper c-TF-IDF formular\n",
        "\n",
        "other_words_to_ignore = ['2025', 'com', 'us', '000', 'ca', 'ch', '10', '41', '91',\n",
        "                         'bj', 'kakanakou', 'miguel', 'stephane', '94085', '06',\n",
        "                         'maude', 'asin', '1zwnj000', 'sarl', '12', '30', 'via',\n",
        "                         'one', 'www', 'fcfa', '38', 'firebasemiguel', 'two',\n",
        "                         'first', 'd68d8', 'rtdb', '94043', '1600', 'pkwy', '44',\n",
        "                         '1855', 'rtdblearn', 'amphitheatre', 'pleasecontact',\n",
        "                         'viewersviewers', 'rise1', '100', 'regardingyour',\n",
        "                         'projectmanage', 'rulesmigueldatabasemiguel', 'ruleswe',\n",
        "                         'ürich', 'skakanakou', 'hasinsecure', 'recruiter1', 'ag',\n",
        "                         'databasecan', 'databasewithout', 'ci', '50', 'solutions1',\n",
        "                         'sunnyvale', 'west', 'trademarks', 'avenue', 'mosh']\n",
        "\n",
        "stop_words_arr = []\n",
        "stop_words_arr.extend(stopwords.words('english'))\n",
        "stop_words_arr.extend(stopwords.words('french'))\n",
        "stop_words_arr.extend(stopwords.words('german'))\n",
        "stop_words_arr.extend(other_words_to_ignore)\n",
        "\n",
        "\n",
        "# Cluster count\n",
        "# Substracting 1 because there is a cluster with ID -1\n",
        "cluster_count = len(set(clusters)) - 1\n",
        "\n",
        "# Cluster content\n",
        "cluster_contents = []\n",
        "for cluster_id in range(cluster_count):\n",
        "  current_cluster_content_arr = df[df['cluster_id'] == cluster_id]['body'].tolist()\n",
        "  current_cluster_content = ' '.join(current_cluster_content_arr)\n",
        "  cluster_contents.append(current_cluster_content)\n",
        "\n",
        "\n",
        "# Bag of words\n",
        "count_vectorizer = CountVectorizer(stop_words=stop_words_arr, max_features=10_000)\n",
        "count_result = count_vectorizer.fit_transform(cluster_contents).toarray()\n",
        "vocabulary = count_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Compute the average number of words per class\n",
        "average_words_per_cluster = sum(sum(count_result)) / len(cluster_contents)\n",
        "\n",
        "# Compute c-tf-idf\n",
        "# Final expected shape is [number_of_cluster, vocabulary_size]\n",
        "# If the word w is not in the cluster, we set its c-TF-IDF value to -1\n",
        "c_tf_idf_result = []\n",
        "for cluster_id in range(cluster_count):\n",
        "  current_cluster_c_tf_idf = []\n",
        "  for word_idx in range(vocabulary.shape[0]):\n",
        "    c_tf = count_result[cluster_id][word_idx]\n",
        "    if c_tf == 0:\n",
        "      current_cluster_c_tf_idf.append(-1.0)\n",
        "    else:\n",
        "      c_tf_idf = average_words_per_cluster / sum(count_result[:, word_idx])\n",
        "      c_tf_idf = math.log(1 + c_tf_idf)\n",
        "      c_tf_idf = c_tf * c_tf_idf\n",
        "      current_cluster_c_tf_idf.append(c_tf_idf)\n",
        "  c_tf_idf_result.append(current_cluster_c_tf_idf)\n",
        "c_tf_idf_result = np.array(c_tf_idf_result)\n",
        "\n",
        "\n",
        "# Get the first top 20 topics per cluster\n",
        "top_n = 20\n",
        "k_th = -1 * top_n\n",
        "top_words_indexes = np.argsort(c_tf_idf_result, axis=1)[:, k_th:][:, ::-1]\n",
        "top_words = np.take(vocabulary, top_words_indexes)\n",
        "print(top_words)\n"
      ],
      "metadata": {
        "id": "Db4YdIv1CT9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aece733e-4bc8-4314-f30f-d948fe5f4c21"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['schule' 'escola' 'einstellungen' 'nachricht' 'planzer' 'falls'\n",
            "  'adliswil' 'interactive' 'paket' 'brokers' 'sendung' 'template' 'app'\n",
            "  'folgenden' 'email' 'mitgeliefert' 'angezeigter' 'mailweiterleitung'\n",
            "  'beantworten' 'benachrichtigungen']\n",
            " ['linkedin' 'email' 'engineer' 'corporation' 'logo' 'intended'\n",
            "  'included' 'unsubscribe' 'notification' 'registered' 'receiving'\n",
            "  'emails' 'java' 'group' 'software' 'learn' 'medium' 'searches' 'missed'\n",
            "  'found']\n",
            " ['amazon' 'coop' 'goods' 'cancellation' 'service' 'mail' 'contract'\n",
            "  'please' 'newsletter' 'delivery' 'order' 'inc' 'chf' 'europe'\n",
            "  'luxembourg' 'eur' 'right' 'plus' 'adresse' 'kennedy']\n",
            " ['nvidia' 'courses' 'new' 'learn' 'css' 'learning' 'frontend' 'masters'\n",
            "  'build' 'skills' 'course' 'certification' 'get' 'year' 'email'\n",
            "  'updates' 'hands' 'time' 'code' 'javascript']\n",
            " ['linkedin' 'new' 'business' 'work' 'help' 'people' 'image' 'get'\n",
            "  'microsoft' 'like' 'google' 'time' 'openai' 'tools' 'next' 'year'\n",
            "  'world' 'way' 'career' 'gemini']\n",
            " ['data' 'system' 'team' 'engineering' 'database' 'like' 'design' 'time'\n",
            "  'service' 'systems' 'user' 'across' 'services' 'new' 'post' 'api'\n",
            "  'process' 'platform' 'key' 'details']\n",
            " ['models' 'model' 'use' 'claude' 'code' 'agent' 'top' 'using' 'open'\n",
            "  'alpha' 'agents' 'work' 'google' 'news' 'openai' 'gemini' 'signal'\n",
            "  'source' 'reasoning' 'research']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Labels from the topics"
      ],
      "metadata": {
        "id": "mgrwlWZJ6A_d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## List of available models"
      ],
      "metadata": {
        "id": "v37VuVAxHZdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for model in client.models.list():\n",
        "  print(model.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUD0xU0RHJ6f",
        "outputId": "32954baa-860a-449e-e579-9ce42ed5e730"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/embedding-gecko-001\n",
            "models/gemini-2.5-flash\n",
            "models/gemini-2.5-pro\n",
            "models/gemini-2.0-flash-exp\n",
            "models/gemini-2.0-flash\n",
            "models/gemini-2.0-flash-001\n",
            "models/gemini-2.0-flash-exp-image-generation\n",
            "models/gemini-2.0-flash-lite-001\n",
            "models/gemini-2.0-flash-lite\n",
            "models/gemini-2.0-flash-lite-preview-02-05\n",
            "models/gemini-2.0-flash-lite-preview\n",
            "models/gemini-exp-1206\n",
            "models/gemini-2.5-flash-preview-tts\n",
            "models/gemini-2.5-pro-preview-tts\n",
            "models/gemma-3-1b-it\n",
            "models/gemma-3-4b-it\n",
            "models/gemma-3-12b-it\n",
            "models/gemma-3-27b-it\n",
            "models/gemma-3n-e4b-it\n",
            "models/gemma-3n-e2b-it\n",
            "models/gemini-flash-latest\n",
            "models/gemini-flash-lite-latest\n",
            "models/gemini-pro-latest\n",
            "models/gemini-2.5-flash-lite\n",
            "models/gemini-2.5-flash-image-preview\n",
            "models/gemini-2.5-flash-image\n",
            "models/gemini-2.5-flash-preview-09-2025\n",
            "models/gemini-2.5-flash-lite-preview-09-2025\n",
            "models/gemini-3-pro-preview\n",
            "models/gemini-3-flash-preview\n",
            "models/gemini-3-pro-image-preview\n",
            "models/nano-banana-pro-preview\n",
            "models/gemini-robotics-er-1.5-preview\n",
            "models/gemini-2.5-computer-use-preview-10-2025\n",
            "models/deep-research-pro-preview-12-2025\n",
            "models/embedding-001\n",
            "models/text-embedding-004\n",
            "models/gemini-embedding-exp-03-07\n",
            "models/gemini-embedding-exp\n",
            "models/gemini-embedding-001\n",
            "models/aqa\n",
            "models/imagen-4.0-generate-preview-06-06\n",
            "models/imagen-4.0-ultra-generate-preview-06-06\n",
            "models/imagen-4.0-generate-001\n",
            "models/imagen-4.0-ultra-generate-001\n",
            "models/imagen-4.0-fast-generate-001\n",
            "models/veo-2.0-generate-001\n",
            "models/veo-3.0-generate-001\n",
            "models/veo-3.0-fast-generate-001\n",
            "models/veo-3.1-generate-preview\n",
            "models/veo-3.1-fast-generate-preview\n",
            "models/gemini-2.5-flash-native-audio-latest\n",
            "models/gemini-2.5-flash-native-audio-preview-09-2025\n",
            "models/gemini-2.5-flash-native-audio-preview-12-2025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_ID = \"gemini-2.5-flash-lite\"\n",
        "# MODEL_ID = \"gemini-3-flash-preview\"\n",
        "# MODEL_ID = \"gemma-3-27b-it\"\n",
        "GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "client = genai.Client(api_key=GEMINI_API_KEY)\n",
        "\n",
        "prompt_template = \"\"\"I will provide a list of keywords. Your task is to\n",
        "generate a single, concise label (1–3 words) that accurately represents the\n",
        "overarching theme or category of these words. The label should be easy to understand.\n",
        "\n",
        "Keywords: {}\n",
        "\n",
        "Think before answering but Output ONLY the label. Do not include introductory\n",
        "text, quotes, or explanations.\"\"\"\n",
        "email_labels = []\n",
        "total_tokens_arr = []\n",
        "\n",
        "for cluster_id in range(cluster_count):\n",
        "  prompt = prompt_template.format(', '.join(top_words[cluster_id].tolist()))\n",
        "  response = client.models.generate_content(\n",
        "      model=MODEL_ID,\n",
        "      contents=prompt)\n",
        "  email_labels.append(response.text)\n",
        "  total_tokens_arr.append(response.usage_metadata.total_token_count)\n",
        "\n",
        "print(email_labels)\n",
        "print(total_tokens_arr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44zb3Z6W5aq3",
        "outputId": "96d3fb14-0f75-406f-b00e-8812d95bc6fa"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Messaging & School Apps', 'Digital Communication', 'Online Shopping', 'Frontend Skills', 'Future Work', 'Data Systems Engineering', 'AI Models']\n",
            "[136, 117, 120, 117, 118, 118, 119]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "no_cluster_id = -1\n",
        "emails_clusters_mapping = []\n",
        "for cluster_id in clusters:\n",
        "  if cluster_id == no_cluster_id:\n",
        "    emails_clusters_mapping.append('NO CLUSTER')\n",
        "  else:\n",
        "    emails_clusters_mapping.append(email_labels[cluster_id])\n",
        "\n",
        "df['cluster_name'] = emails_clusters_mapping"
      ],
      "metadata": {
        "id": "aJ16SYZmHXah"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}